{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7H420KxmCjKH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCDUFCh7Fd-n"
   },
   "outputs": [],
   "source": [
    "# To add your own Drive Run this cell.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQXiXrbaF3NK"
   },
   "outputs": [],
   "source": [
    "# Please append your own directory after â€˜/content/drive/My Drive/'\n",
    "### ========== TODO : START ========== ###\n",
    "sys.path += ['/content/drive/My Drive/cm146-spring23/hw3/HW3-code']\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7_OLupUPC2U3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author      : Yi-Chieh Wu, Sriram Sankararman\n",
    "Description : Twitter\n",
    "\"\"\"\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# !!! MAKE SURE TO USE LinearSVC.decision_function(X), NOT LinearSVC.predict(X) !!!\n",
    "# (this makes ''continuous-valued'' predictions)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47L2XVzBX6c5"
   },
   "source": [
    "# Problem 3: Twitter Analysis Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Z8E5YL0CzWe"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "\n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "\n",
    "    np.savetxt(outfile, vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i67aTAmrGGHi"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "\n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "\n",
    "    word_list = {}\n",
    "    idx = 0\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate word_list\n",
    "        for input_string in fid:\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                if word not in word_list:\n",
    "                    word_list[word] = idx\n",
    "                    idx += 1\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = sum(1 for line in open(infile,'r'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate feature_matrix\n",
    "        for i, input_string in enumerate(fid):\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                feature_matrix[i, word_list[word]] = 1.0\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-MvTxQPRGOOf"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the\n",
    "    true labels and the predicted labels.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1a: compute classifier performance\n",
    "    perf = None\n",
    "    if metric==\"accuracy\":\n",
    "        perf = metrics.accuracy_score(y_true=y_true, y_pred=y_label)\n",
    "    if metric==\"f1-score\":\n",
    "        perf = metrics.f1_score(y_true=y_true, y_pred=y_label)\n",
    "    if metric==\"auroc\":\n",
    "        perf = metrics.roc_auc_score(y_true=y_true, y_pred=y_label)\n",
    "    if metric==\"precision\":\n",
    "        perf = metrics.precision_score(y_true=y_true, y_pred=y_label)\n",
    "    if metric==\"sensitivity\":\n",
    "        perf = metrics.recall_score(y_true=y_true, y_pred=y_label)\n",
    "    if metric==\"specificity\":\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true=y_true, y_pred=y_label).ravel()\n",
    "        perf = tn /(tn + fp)\n",
    "    return perf\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of LinearSVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: compute average cross-validation performance\n",
    "    folds = kf.get_n_splits(X, y)\n",
    "\n",
    "\n",
    "    p = 0\n",
    "    for i, (train, test) in enumerate( kf.splits(X,y)):\n",
    "            clf.fit(X[train], y[train])\n",
    "            p += performance(y[test], clf.decision_function(X[test]))\n",
    "            \n",
    "    return p / folds\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear SVM\n",
    "    \"\"\"\n",
    "\n",
    "    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1c: select optimal hyperparameter using cross-validation\n",
    "    best = -1\n",
    "    c_val = -1\n",
    "    \n",
    "    for c in C_range:\n",
    "        acc = cv_performance(clf = LinearSVC(loss = 'hinge', random_state= 0, C=c), X=X, y=y, kf = kf)\n",
    "        if acc > best:\n",
    "            best = acc\n",
    "            c_val = c\n",
    "    return c_val\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of LinearSVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: return performance on test data under a metric.\n",
    "    y_pred = clf.decision_function(X)\n",
    "    score = performance(y_true=y, y_pred=y_pred, metric=metric)\n",
    "    return score\n",
    "    ### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMIQRGpYErVF"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/cm146-spring23/hw3/HW3-code/data/tweets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m### ========== TODO : END ========== ###\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m :\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m label_path = \u001b[33m'\u001b[39m\u001b[33m/content/drive/My Drive/cm146-spring23/hw3/HW3-code/data/labels.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m### ========== TODO : END ========== ###\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m dictionary = \u001b[43mextract_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dictionary))\n\u001b[32m     15\u001b[39m X = extract_feature_vectors(file_path, dictionary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mextract_dictionary\u001b[39m\u001b[34m(infile)\u001b[39m\n\u001b[32m     38\u001b[39m word_list = {}\n\u001b[32m     39\u001b[39m idx = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fid :\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# process each line to populate word_list\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m input_string \u001b[38;5;129;01min\u001b[39;00m fid:\n\u001b[32m     43\u001b[39m         words = extract_words(input_string)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.4/envs/ece-m146-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/My Drive/cm146-spring23/hw3/HW3-code/data/tweets.txt'"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# main\n",
    "######################################################################\n",
    "\n",
    "def main() :\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # read the tweets and its labels, change the following two lines to your own path.\n",
    "    ### ========== TODO : START ========== ###\n",
    "    file_path = '/Users/divikchotani/github/ece-m146/HW3-code/data/labels.txt'\n",
    "    label_path = '/Users/divikchotani/github/ece-m146/HW3-code/data/tweets.txt'\n",
    "    ### ========== TODO : END ========== ###\n",
    "    dictionary = extract_dictionary(file_path)\n",
    "    print(len(dictionary))\n",
    "    X = extract_feature_vectors(file_path, dictionary)\n",
    "    y = read_vector_file(label_path)\n",
    "    # split data into training (training + cross-validation) and testing set\n",
    "    X_train, X_test = X[:560], X[560:]\n",
    "    y_train, y_test = y[:560], y[560:]\n",
    "\n",
    "    metric_list = [\"accuracy\", \"f1-score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: create stratified folds (5-fold CV)\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    # part 1c: for each metric, select optimal hyperparameter for linear SVM using CV\n",
    "    cs = []\n",
    "    for metric in metric_list:\n",
    "        cs.append( select_param_linear(X=X_train, y=y_train, kf=kf, metric=metric))\n",
    "    # part 2a: train linear SVMs with selected hyperparameters\n",
    "    clfs = []\n",
    "    for i in range(6):\n",
    "        t = LinearSVC(loss = 'hinge', random_state= 0, C=cs[i]).fit(X = X_train, y = y_train)\n",
    "        t.pre\n",
    "        clfs.append(t)\n",
    "    # part 2b: test the performance of your classifiers.\n",
    "    for i, clf in enumerate(clfs):\n",
    "        print(metric_list[i], performance_test(clf=clf, X = X_test, y=y_test, metric= metric_list[i]))\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W-_mjX0JMes"
   },
   "source": [
    "# Problem 4: Boosting vs. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uzCdPTkOQSY"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVxef2sxOmVI"
   },
   "outputs": [],
   "source": [
    "class Data :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \"\"\"\n",
    "        Data class.\n",
    "        \n",
    "        Attributes\n",
    "        --------------------\n",
    "            X -- numpy array of shape (n,d), features\n",
    "            y -- numpy array of shape (n,), targets\n",
    "        \"\"\"\n",
    "                \n",
    "        # n = number of examples, d = dimensionality\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.Xnames = None\n",
    "        self.yname = None\n",
    "    \n",
    "    def load(self, filename, header=0, predict_col=-1) :\n",
    "        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n",
    "        \n",
    "        # determine filename\n",
    "        f = filename\n",
    "        \n",
    "        # load data\n",
    "        with open(f, 'r') as fid :\n",
    "            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n",
    "        \n",
    "        # separate features and labels\n",
    "        if predict_col is None :\n",
    "            self.X = data[:,:]\n",
    "            self.y = None\n",
    "        else :\n",
    "            if data.ndim > 1 :\n",
    "                self.X = np.delete(data, predict_col, axis=1)\n",
    "                self.y = data[:,predict_col]\n",
    "            else :\n",
    "                self.X = None\n",
    "                self.y = data[:]\n",
    "        \n",
    "        # load feature and label names\n",
    "        if header != 0:\n",
    "            with open(f, 'r') as fid :\n",
    "                header = fid.readline().rstrip().split(\",\")\n",
    "                \n",
    "            if predict_col is None :\n",
    "                self.Xnames = header[:]\n",
    "                self.yname = None\n",
    "            else :\n",
    "                if len(header) > 1 :\n",
    "                    self.Xnames = np.delete(header, predict_col)\n",
    "                    self.yname = header[predict_col]\n",
    "                else :\n",
    "                    self.Xnames = None\n",
    "                    self.yname = header[0]\n",
    "        else:\n",
    "            self.Xnames = None\n",
    "            self.yname = None\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def load_data(filename, header=0, predict_col=-1) :\n",
    "    \"\"\"Load csv file into Data class.\"\"\"\n",
    "    data = Data()\n",
    "    data.load(filename, header=header, predict_col=predict_col)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Zcf4WVqJSpe"
   },
   "outputs": [],
   "source": [
    "# Change the path to your own data directory\n",
    "### ========== TODO : START ========== ###\n",
    "titanic = load_data(\"/content/drive/My Drive/cm146-spring23/hw3/HW3-code/data/titanic_train.csv\", header=1, predict_col=0)\n",
    "### ========== TODO : END ========== ###\n",
    "X = titanic.X; Xnames = titanic.Xnames\n",
    "y = titanic.y; yname = titanic.yname\n",
    "n,d = X.shape  # n = number of examples, d =  number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ta7XHRWQGNo"
   },
   "outputs": [],
   "source": [
    "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
    "    \"\"\"\n",
    "    Computes the classifier error over a random split of the data,\n",
    "    averaged over ntrials runs.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf         -- classifier\n",
    "        X           -- numpy array of shape (n,d), features values\n",
    "        y           -- numpy array of shape (n,), target classes\n",
    "        ntrials     -- integer, number of trials\n",
    "        test_size   -- proportion of data used for evaluation\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        train_error -- float, training error\n",
    "        test_error  -- float, test error\n",
    "    \"\"\"\n",
    "\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "\n",
    "    train_scores = []; test_scores = [];\n",
    "    for i in range(ntrials):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split (X,y, test_size = test_size, random_state = i)\n",
    "        clf.fit (xtrain, ytrain)\n",
    "\n",
    "        ypred = clf.predict (xtrain)\n",
    "        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n",
    "        train_scores.append (err)\n",
    "\n",
    "        ypred = clf.predict (xtest)\n",
    "        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n",
    "        test_scores.append (err)\n",
    "\n",
    "    train_error =  np.mean (train_scores)\n",
    "    test_error = np.mean (test_scores)\n",
    "    return train_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8-U3un5PjGq"
   },
   "outputs": [],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(a): Implement the decision tree classifier and report the training error.\n",
    "print('Classifying using Decision Tree...')\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x_PevK8Q4dx"
   },
   "outputs": [],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n",
    "print('Classifying using Random Forest...')\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFUyPTPwT53v"
   },
   "outputs": [],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n",
    "print('Classifying using Random Forest...')\n",
    "### ========== TODO : END ========== ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ece-m146-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
