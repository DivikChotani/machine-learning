{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: A Two-Layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (50000, 576)\n",
      "Train target shape:  (50000,)\n",
      "Val data shape:  (10000, 576)\n",
      "Val target shape:  (10000,)\n",
      "Test data shape:  (10000, 576)\n",
      "Test target shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Load fashionMNIST. This is the same code with homework 1.\n",
    "## \n",
    "def crop_center(img,cropped):\n",
    "    img = img.reshape(-1, 28, 28)\n",
    "    start = 28//2-(cropped//2)   \n",
    "    img = img[:, start:start+cropped, start:start+cropped]\n",
    "    return img.reshape(-1, cropped*cropped)\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'% kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), 'B', offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(),'B', offset=16).reshape(-1, 784)\n",
    "        images = crop_center(images, 24)\n",
    "    return images, labels\n",
    "X_train_and_val, y_train_and_val = load_mnist('./data/mnist', kind='train')\n",
    "X_test, y_test = load_mnist('./data/mnist', kind='t10k')\n",
    "X_train, X_val = X_train_and_val[:50000], X_train_and_val[50000:]\n",
    "y_train, y_val = y_train_and_val[:50000], y_train_and_val[50000:]\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Val data shape: ', X_val.shape)\n",
    "print('Val target shape: ', y_val.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is Odd\n"
     ]
    }
   ],
   "source": [
    "# PART (a): \n",
    "# To Visualize a point in the dataset\n",
    "index = 10\n",
    "X = np.array(X_train[index], dtype='uint8').reshape([24, 24])\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "if y_train[index] in set([1, 3, 5, 7, 9]):\n",
    "    label = 'Odd'\n",
    "else:\n",
    "    label = 'Even'\n",
    "print('Label is', label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, you will build a two-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary label\n",
    "y_train = y_train.astype(int) % 2\n",
    "y_val = y_val.astype(int) % 2\n",
    "y_test = y_test.astype(int) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network for binary classification. \n",
    "    We train the network with a softmax output and cross entropy loss function \n",
    "    with L2 regularization on the weight matrices. The network uses a ReLU \n",
    "    nonlinearity after the first fully connected layer.\n",
    "    Input: X\n",
    "    Hidden states for layer 1: h1 = XW1 + b1\n",
    "    Activations: a1 = ReLU(h1)\n",
    "    Hidden states for layer 2: h2 = a1W2 + b2\n",
    "    Probabilities: s = softmax(h2)\n",
    "    \n",
    "    ReLU function: \n",
    "    (i) x = x if x >= 0  (ii) x = 0 if x < 0\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the output of the neural network using forward pass.  \n",
    "        #   The result should have (N, C), where N is the number of examples, and C is the number \n",
    "        #   of classes. \n",
    "        #   The output of the second fully connected layer is the output scores (before softmax). \n",
    "        #   Do not use a for loop in your implementation.\n",
    "        #   Please use 'h1' as input of hidden layers, and 'a1' as output of \n",
    "        #   Refer to the comments at the beginning of this class for the model architecture\n",
    "        #   You may simply use np.maximun for implementing ReLU.\n",
    "        ##  Part (b): Implement the forward pass and compute scores.\n",
    "        \n",
    "        h1 = X.dot(W1) + b1\n",
    "        a1 = np.maximum(0, h1)\n",
    "\n",
    "        scores = a1.dot(W2) +b2\n",
    "\n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "\n",
    "        # scores is num_examples by num_classes (N, C)\n",
    "        def softmax_loss(x, y):\n",
    "            ### ========== TODO : START ========== ###\n",
    "            #   Calculate the cross entropy loss after softmax output layer.\n",
    "            #   This function should return loss and dx\n",
    "            probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "            N = x.shape[0]\n",
    "            ##  Part (d): Implement the CrossEntropyLoss\n",
    "            loss = None\n",
    "            logprobs = -np.log(probs[np.arange(N), y])\n",
    "            loss = np.sum(logprobs) / N\n",
    "            ##  Part (d): Implement the gradient of y wrt x\n",
    "            dx = None\n",
    "            dx = probs[:]\n",
    "            dx[np.arange(N), y] -= 1\n",
    "            dx /= N\n",
    "            ### ========== TODO : END ========== ###\n",
    "            return loss, dx\n",
    "        \n",
    "        \n",
    "        data_loss, dscore = softmax_loss(scores, y) \n",
    "        \n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the regularization loss. Multiply the regularization\n",
    "        #   loss by 0.5 (in addition to the factor reg).\n",
    "        ##  Part (c): Implement the regularization loss\n",
    "        reg_loss = None\n",
    "        reg_loss = 0.5* reg*np.sum(np.sum(W1**2)+ np.sum(W2**2))\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        grads = {}\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #  Compute backpropagation\n",
    "        #  Remember the loss contains two parts: cross-entropy and regularization. The computation for gradients of W1 and b1 shown here can be regarded as a reference.\n",
    "        ## Part (e): Implement the computations of gradients for W2 and b2.\n",
    "        grads['W2'] = None\n",
    "        grads['b2'] = None\n",
    "        \n",
    "        dh = np.dot(dscore, W2.T)\n",
    "        dh[a1 <= 0] = 0\n",
    "\n",
    "        grads['W1'] = np.dot(X.T, dh) + reg * W1\n",
    "        grads['b1'] = np.ones(N).dot(dh)\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "          X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in np.arange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #   Create a minibatch (X_batch, y_batch) by sampling batch_size \n",
    "            #   samples randomly.\n",
    "\n",
    "            b_index = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[b_index]\n",
    "            y_batch = y[b_index]\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            \n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['b1'] -= learning_rate * grads['b1']\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b2'] -= learning_rate * grads['b2']\n",
    "\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "          classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "          to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Predict the class given the input data.\n",
    "        ##  Part (f): Implement the prediction function\n",
    "        \n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 576\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "for learning_rate in [1e-5, 1e-4, 1e-3, 5e-3, 1e-1]:\n",
    "  print('learning_rate: ', learning_rate)\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=200,\n",
    "              learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "              reg=0.1, verbose=True)\n",
    "\n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  # Save this net as the variable subopt_net for later comparison.\n",
    "  subopt_net = net\n",
    "  test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "  print('Test accuracy (subopt_net): ', test_acc)\n",
    "  print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to load the CIFAR10 data\n",
    "## Documentation of CIFAR10: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def dataloader():\n",
    "  import tensorflow as tf\n",
    "  cifar10 = tf.keras.datasets.cifar10\n",
    "  (_, _), (X, y) = cifar10.load_data()\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple utility function to visualize the data\n",
    "def visualize(X, ind):\n",
    "  from PIL import Image \n",
    "  plt.imshow(Image.fromarray(X[ind], 'RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10K images of size 32 x 32 x 3 \n",
    "# where 32 x 32 is the height and width of the image\n",
    "# 3 is the number of channels 'RGB'\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Implement this function to form a 10000 x N matrix \n",
    "  from 10000 x 32 x 32 x 3 shape input.\n",
    "'''\n",
    "def reshape(X):\n",
    "  '''\n",
    "    Write one line of code here\n",
    "  '''\n",
    "  ### ========== TODO : START ========== ###\n",
    "  # part (a)\n",
    "\n",
    "  ### ========== TODO : END ========== ###\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reshape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_score = []\n",
    "for i in tqdm(range(5, 20, 5)):\n",
    "  score = 0\n",
    "  for rs in tqdm(range(3)):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'random', random_state = rs)\n",
    "    '''\n",
    "      Write one line of code to fit the kMeans algorithm to the data\n",
    "      Write another line of code to report the kMeans clustering score \n",
    "      defined as sum of squared distances of samples to their closest \n",
    "      cluster center, weighted by the sample weights if provided.\n",
    "      Hint: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    '''\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part (b)\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "  clustering_score.append(score/3) ## divide by 3 because 3 random states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Submit the plot you get after running this piece of code in your solutions\n",
    "'''\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(5, 20, 5), clustering_score)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('Clustering Score')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize K Clusters for K = 10 and random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2) \n",
    "#Transform the data\n",
    "df = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyzing the input data in 2D based on its true labels\n",
    "\n",
    "u_labels = np.unique(y[:, 0])\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(df[y[:, 0] == i , 0] , df[y[:, 0] == i , 1] , label = i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Submit the output plot as a part of the solutions\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters = 10, init = 'random', random_state = 42)\n",
    "'''\n",
    "  Write 1 - 2 line of code to get the predicted labels of the 10-clusters\n",
    "'''\n",
    "### ========== TODO : START ========== ###\n",
    "\n",
    "### ========== TODO : END ========== ###\n",
    "\n",
    "u_labels = np.unique(label)\n",
    "\n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels:\n",
    "    '''\n",
    "      Write one line of code to get a scatter plot for i-th cluster.\n",
    "      Have its label = i\n",
    "    '''\n",
    "    ### ========== TODO : START ========== ###\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece-m146-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
